opencv-python
numpy
pandas
matplotlib
tensorflow
tensorflow-cpu
keras_preprocessing
scikit_learn
tqdm
seaborn
scikit-multilearn

##################### FIX UNBALANCED TRAIN DATA #####################


print ('original number of classes: ', len(df['label'].unique()))
size=300 # set number of samples for each class
samples=[]
group=df.groupby('label')
for label in df['label'].unique():
    Lgroup=group.get_group(label)
    count=int(Lgroup['label'].value_counts())    
    if count>=size: # 300
        sample=Lgroup.sample(size, axis=0)        
    else:        
        sample=Lgroup.sample(frac=1, axis=0)
    samples.append(sample) 
train_df=pd.concat(samples, axis=0).reset_index(drop=True)
print (len(train_df))
print ('final number of classes: ', len(train_df['label'].unique()))       
print (train_df['label'].value_counts())  


##################### TEST AND TRAIN SPLIT #####################

train_split=.80 # percentage of data used for training
valid_split=.10 # percentage of data used for validation

# percentage of data used for test is 1-train_split-valid_split 
test_val_split = valid_split/(1-train_split) # split of 0.5 

# Splitting data into train df and remaining data
train_df, remaining_data = train_test_split(df, train_size=train_split, shuffle=True, random_state=666)

# Splitting remaining data into validation and test df
val_df, test_df=train_test_split(remaining_data, train_size= test_val_split, shuffle=True, random_state=666)

# Printing splits
print( "***"*15)
print('Current data split: ')  
print( "---"*15)
print('train_df length: ', len(train_df))  
print( "---"*15)
print('test_df length: ', len(test_df))
print( "---"*15)
print( 'valid_df length: ', len(val_df))
print( "***"*15)


# The data is unbalanced with regards to labels 
print(train_df['label'].value_counts())
print(test_df['label'].value_counts())
print(val_df['label'].value_counts())


##################### FIX UNBALANCED TRAIN DATA #####################


print ('original number of classes: ', len(df['label'].unique()))
size=300 # set number of samples for each class
samples=[]
group=df.groupby('label')
for label in df['label'].unique():
    Lgroup=group.get_group(label)
    count=int(Lgroup['label'].value_counts())    
    if count>=size: # 300
        sample=Lgroup.sample(size, axis=0)        
    else:        
        sample=Lgroup.sample(frac=1, axis=0)
    samples.append(sample) 
train_df=pd.concat(samples, axis=0).reset_index(drop=True)
print (len(train_df))
print ('final number of classes: ', len(train_df['label'].unique()))       
print (train_df['label'].value_counts())  





